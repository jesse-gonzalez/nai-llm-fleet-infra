k8s_cluster:

  ## kubernetes distribution - supported "nke" "kind"
  distribution: kind
  ## kubernetes cluster name
  name: flux-kind-local
  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)
  profile: kind-stacks
  ## environment name - based on environment selected (e.g., prod, non-prod, etc.)
  environment: kserve

  ## docker hub registry congis
  registry:
    docker_hub:
      user: _required
      password: _required

  ## nvidia gpu specific configs
  gpu_operator:
    enabled: false
    version: v23.9.0
   # cuda_toolkit_version: v1.14.3-centos7
   # time_slicing:
   #   enabled: false
   #   replica_count: 2

flux:
  ## flux specific configs for github repo
  github:
    repo_url: _required
    repo_user: _required
    repo_api_token: _required

infra:
  ## Global nutanix configs
  nutanix:
    ## Nutanix Prism Creds
    prism_central:
      enabled: false
      # endpoint: _required
      # user: admin
      # password: _required

    ## Nutanix Objects Store Configs
    objects:
      enabled: false
      # host: _required
      # port: _required
      # region: _required
      # use_ssl: false
      # access_key: _required
      # secret_key: _required

services:
  #####################################################
  ## Required variables for kube-vip and depedent services
  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses
  kube_vip:
    enabled: true
    ## min. 2 ips for range (i.e., 10.38.110.22-10.38.110.23)
    ipam_range: 172.20.0.22-172.20.0.23

  ## required for all platform services that are leveraging nginx-ingress
  nginx_ingress:
    enabled: true
    version: 4.8.3
    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. 
    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer
    vip: 172.20.0.20
    
    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster 
    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.
    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com
    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates
    wildcard_ingress_subdomain: flux-kind-local.172.20.0.20.nip.io

    ## Wildcard Ingress Subdomain for management cluster.
    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates
    management_cluster_ingress_subdomain: flux-kind-local.172.20.0.20.nip.io

  ## required for clusters/_profiles that leverage kserve / knative-serving / istio components
  istio:
    enabled: true
    version: "1.17.2"
    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. 
    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer
    ## This address should be mapped to wildcard_ingress_subdomain defined below
    vip: 172.20.0.21

    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. 
    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm.example.com, then value is llm.example.com
    ## If leveraging AWS Route 53 DNS with Let's Encrypt (below), make sure to enable/configure AWS credentials needed to 
    ## support CertificateSigningRequests using ACME DNS Challenges.
    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. For Example: llm.flux-kind-local.172.20.0.21.nip.io
    wildcard_ingress_subdomain: llm.flux-kind-local.172.20.0.21.nip.io

  cert_manager:
    ## if enabled - cluster issuer will be self-signed-issuer
    enabled: true
    version: v1.3.0
    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to "letsencrypt-issuer"
    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge
    aws_route53_acme_dns:
      enabled: false
    #   email: _required
    #   zone: _required
    #   region: _required
    #   hosted_zone_id: _required
    #   key_id: _required
    #   key_secret: _required
  
  ## do not disable kyverno unless you know what you're doing
  kyverno:
    enabled: true
    version: 3.1.4

  ## other platform config defaults. you can override versions for each service here.
  ## do not remove these keys in interim, they are still required

  kserve:
    enabled: true
    version: v0.11.2

  knative_serving:
    enabled: true
    version: knative-v1.10.1

  knative_istio:
    enabled: true
    version: knative-v1.10.0

  knative_eventing:
    enabled: true
    version: knative-v1.10.1

  kafka:
    enabled: true
    version: 26.8.5

  kubernetes_dashboard:
    enabled: false
    version: 7.3.2

  milvus:
    enabled: false
    version: 4.1.13
    milvus_bucket_name: milvus

  opentelemetry_collector:
    enabled: false
    version: 0.80.1

  opentelemetry_operator:
    enabled: false
    version: 0.47.0

  uptrace:
    enabled: false
    version: 1.5.7

  weave_gitops:
    enabled: true
    version: 4.0.36

  jupyterhub:
    enabled: false
    version: 3.1.0

  redis:
    enabled: false
    version: 18.1.6

  elasticsearch:
    enabled: false
    version: 19.13.10

apps:
  ## Required GPT NVD Reference Application Helm Chart Configs
  gptnvd_reference_app:
    enabled: false
    version: 0.2.7
    #milvus_bucket_name: milvus
    #documents_bucket_name: documents01
  ## Required NAI LLM Helm Chart Configs
  nai_helm:
    enabled: false
    version: 0.1.1
    # model: llama2_7b_chat
    # revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8
    # useExistingNFS: true
    # nfs_export: /llm-model-store
    # nfs_server: 10.38.110.47
    # maxTokens: 4000
    # repPenalty: 1.2
    # temperature: 0.2
    # topP: 0.9
    # huggingFaceToken: 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

